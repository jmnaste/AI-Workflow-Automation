# Engineering Method — Vibe Coding + Quality Gates

## Purpose
Define a **smart, simple, and effective framework** for developing, validating, and delivering AI automation projects with **speed**, **discipline**, and **measurable quality**. This methodology combines **Vibe Coding** (fast creative loops) with **Quality Gates** (objective checkpoints) to ensure reliability without slowing innovation.

---

## 1. Core Philosophy
The engineering method is founded on three principles:
1. **Speed with structure:** Move fast but measure everything.
2. **Creativity with accountability:** Encourage iteration, but enforce validation gates.
3. **Clarity over complexity:** Short cycles, visible outcomes, documented intent.

> _"Build in vibe, validate in gates."_

---

## 2. The Vibe Coding Loop
Each development cycle follows a tight, repeatable loop:

### Steps:
1. **Plan:** Define intent and hypothesis.
2. **Build:** Implement or modify a feature, graph node, or automation flow.
3. **Evaluate:** Run tests and golden-set evals; review metrics.
4. **Document:** Update project docs, prompts, and changelogs automatically.
5. **Demo:** Present measurable outcomes visually (dashboard, log, or Loom).

### Typical Duration
1–3 hours per loop (micro-cycle) or 1–2 days for larger deliverables.

---

## 3. Quality Gates
Quality Gates serve as **non-negotiable validation checkpoints** before merging, releasing, or promoting to production.

| Gate | Focus | Pass Criteria |
|------|--------|---------------|
| **Architecture Gate** | Design consistency | Follows defined stack, clear boundaries, no duplication |
| **Code Gate** | Code quality | Linted, typed, tested, and peer-reviewed |
| **Eval Gate** | Functional reliability | ≥ 90% golden tests passing, no regressions |
| **Security Gate** | Compliance & safety | Secrets protected, no PII leaks, restricted tool scopes |
| **Documentation Gate** | Knowledge continuity | Docs and changelogs updated automatically |

> If any gate fails, fix and loop again — never bypass.

---

## 4. Deliverables per Cycle
- **Code Artifact:** Implemented node, route, or connector.
- **Metrics:** Latency, accuracy, resource usage, and pass rate.
- **Demo Evidence:** Short recording or dashboard snapshot.
- **Documentation Update:** Auto-adapted fundamental docs via AI.
- **Commit Tag:** Semantic commit with gate results.

Example Commit:
```
feat(graph-decision): improved confidence routing (+7% eval pass)
```

---

## 5. Observability & Evals Integration
- **Evals:** Golden-set YAML tests automatically run in CI.
- **Observability (workflow-first):** Each change is accompanied by workflow run evidence — run list snapshots, run detail with state timelines, retries, failure reasons, and HITL decisions — with traces/metrics/logs attached for diagnostics.
- **Scorecard:** Each PR gets a numeric health score (code + eval + outcomes).

Business-first KPIs for SMB-facing reporting:
- Time-to-feature and effectiveness trend (throughput, first-pass success, accuracy)
- Effectiveness index (before → after) and error-rate reduction
- Turnaround/response time and cash-cycle improvements
- AI contribution stability & reliability (consistency across runs, drift detection, HITL escalation rate)

Engineering telemetry (internal):
- p95/p99 latency, token/compute usage per run, error codes, and retry counts

Example metrics:
- Time-to-feature ≤ target; effectiveness trend up and to the right
- Error rate reduction ≥ 50% in target workflows; turnaround time −30%+
- Internal SLOs: p95 latency < 2.5s; resource usage/run within budget envelope; eval pass rate ≥ 90%

---

## 6. Automation Support
The Vibe + Gate workflow is partially automated via:
- **Scripts:** `make cycle`, `adapt_docs.py`, `eval_runner.py`.
- **CI/CD:** Runs tests, evals, and doc updates.
- **Dashboards:** Visualize project health (tests, evals, coverage) and workflow run health (success rate, retries, failure reasons) alongside resource usage and latency.

---

## 7. Collaboration Practices
- **Semantic Commits:** `feat:`, `fix:`, `refactor:`, `docs:` prefixes.
- **PR Ritual:** Summary + evidence + demo link.
- **Review Etiquette:** Respect loop; focus on clarity, not verbosity.
- **Async Transparency:** Share demo results after each micro-cycle.

---

## 8. Typical Workflow Example
1. **Define Goal:** "Improve summarizer precision by 5%."
2. **Implement:** Update LangGraph decision node.
3. **Run Evals:** `make eval`; verify accuracy + latency.
4. **Document:** `make docs`; AI updates project overview.
5. **Demo:** Post dashboard screenshot + short Loom.
6. **Merge:** All gates pass → merge → tag release.

---

## 9. Benefits
- **Predictability:** Each loop produces measurable value.
- **Traceability:** Every artifact is linked to metrics and validation evidence.
- **Scalability:** Reusable process across projects and teams.
- **Confidence:** Rapid delivery without technical debt.

---

## 10. Glossary of Technical Terms and Acronyms
| Term | Definition |
|------|-------------|
| **Vibe Coding** | A development method emphasizing short, high-energy cycles of planning, building, evaluating, and documenting. |
| **Quality Gate** | Mandatory checkpoint ensuring outputs meet objective quality criteria. |
| **Eval / Golden Set** | Predefined test cases measuring reliability, accuracy, or consistency of AI workflows. |
| **CI/CD** | Continuous Integration / Continuous Deployment: automated testing and release pipelines. |
| **p95 Latency** | 95th percentile latency — the time within which 95% of requests complete. Public-facing reports treat this as a supporting diagnostic; the major indicator is AI contribution stability & reliability. |
| **PII (Personally Identifiable Information)** | Sensitive user data that must be masked or protected. |
| **LLM (Large Language Model)** | AI model trained on large text datasets for reasoning and text generation. |
| **LangGraph** | Python framework for stateful agentic workflows. |
| **FastAPI** | High-performance Python web framework for building APIs. |
| **Telemetry** | Collection of system data (metrics, logs, traces) for observability. |
| **Semantic Commit** | Git convention that encodes change intent (feat, fix, refactor, etc.). |
| **Monorepo** | A single repository hosting multiple components of the system. |
| **HITL (Human-in-the-Loop)** | Human supervision step for AI validation and correction. |
| **p95 / p99** | Performance percentiles used to measure latency and stability. |
| **AI contribution stability & reliability** | Consistency of AI outputs across runs and time; sensitivity to model/prompt/data drift; rate of HITL escalations and corrections. |
| **KPI (Key Performance Indicator)** | Metric used to evaluate system or project success. |
| **RAG (Retrieval-Augmented Generation)** | Combining document retrieval with AI generation for context-rich answers. |
| **PR (Pull Request)** | Git-based workflow step to review and merge code contributions. |

---

### Summary
The **Vibe + Quality Gates** method unites creativity and control — empowering developers to iterate fast while guaranteeing measurable reliability. Every cycle creates visible progress, every gate enforces trust, and every release builds both confidence and velocity.

