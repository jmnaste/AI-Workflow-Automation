# Engineering Method — Vibe Coding + Quality Gates

## Purpose
Define a **smart, simple, and effective framework** for developing, validating, and delivering AI automation projects with **speed**, **discipline**, and **measurable quality**. This methodology combines **Vibe Coding** (fast creative loops) with **Quality Gates** (objective checkpoints) to ensure reliability without slowing innovation.

---

## 1. Core Philosophy
The engineering method is founded on three principles:
1. **Speed with structure:** Move fast but measure everything.
2. **Creativity with accountability:** Encourage iteration, but enforce validation gates.
3. **Clarity over complexity:** Short cycles, visible outcomes, documented intent.

> _"Build in vibe, validate in gates."_

---

## 2. The Vibe Coding Loop
Each development cycle follows a tight, repeatable loop:

### Steps:
1. **Plan:** Define intent and hypothesis.
2. **Build:** Implement or modify a feature, graph node, or automation flow.
3. **Evaluate:** Run tests and golden-set evals; review metrics.
4. **Document:** Update project docs, prompts, and changelogs automatically.
5. **Demo:** Present measurable outcomes visually (dashboard, log, or Loom).

### Typical Duration
1–3 hours per loop (micro-cycle) or 1–2 days for larger deliverables.

---

## 3. Quality Gates
Quality Gates serve as **non-negotiable validation checkpoints** before merging, releasing, or promoting to production.

| Gate | Focus | Pass Criteria |
|------|--------|---------------|
| **Architecture Gate** | Design consistency | Follows defined stack, clear boundaries, no duplication |
| **Code Gate** | Code quality | Linted, typed, tested, and peer-reviewed |
| **Eval Gate** | Functional reliability | ≥ 90% golden tests passing, no regressions |
| **Security Gate** | Compliance & safety | Secrets protected, no PII leaks, restricted tool scopes |
| **Documentation Gate** | Knowledge continuity | Docs and changelogs updated automatically |

> If any gate fails, fix and loop again — never bypass.

---

## 4. Deliverables per Cycle
- **Code Artifact:** Implemented node, route, or connector.
- **Metrics:** Latency, accuracy, resource usage, and pass rate.
- **Demo Evidence:** Short recording or dashboard snapshot.
- **Documentation Update:** Auto-adapted fundamental docs via AI.
- **Commit Tag:** Semantic commit with gate results.

Example Commit:
```
feat(graph-decision): improved confidence routing (+7% eval pass)
```

---

## 5. Observability & Evals Integration
- **Evals:** Golden-set YAML tests automatically run in CI.
- **Observability (workflow-first):** Each change is accompanied by workflow run evidence — run list snapshots, run detail with state timelines, retries, failure reasons, and HITL decisions — with traces/metrics/logs attached for diagnostics.
- **Scorecard:** Each PR gets a numeric health score (code + eval + outcomes).

Business-first KPIs for SMB-facing reporting:
- Time-to-feature and effectiveness trend (throughput, first-pass success, accuracy)
- Effectiveness index (before → after) and error-rate reduction
- Turnaround/response time and cash-cycle improvements
- AI contribution stability & reliability (consistency across runs, drift detection, HITL escalation rate)

Engineering telemetry (internal):
- p95/p99 latency, token/compute usage per run, error codes, and retry counts

Example metrics:
- Time-to-feature ≤ target; effectiveness trend up and to the right
- Error rate reduction ≥ 50% in target workflows; turnaround time −30%+
- Internal SLOs: p95 latency < 2.5s; resource usage/run within budget envelope; eval pass rate ≥ 90%

---

## 6. Automation Support
The Vibe + Gate workflow is partially automated via:
- **Scripts:** `make cycle`, `adapt_docs.py`, `eval_runner.py`.
- **CI/CD:** Runs tests, evals, and doc updates.
- **Dashboards:** Visualize project health (tests, evals, coverage) and workflow run health (success rate, retries, failure reasons) alongside resource usage and latency.

---

## 7. Collaboration Practices

### Semantic Commits
- **Format:** `type(scope): description`
- **Types:** `feat:`, `fix:`, `refactor:`, `docs:`, `test:`, `chore:`
- **Examples:**
  - `feat(webui): implement login page with JWT auth`
  - `fix(bff): resolve CORS issue in auth middleware`
  - `refactor(api): extract validation logic to shared module`

### Commit Workflow with Copilot
When ready to commit, user says: **"I want to commit and update prompt log"**

Copilot will:
1. Add entry to `docs/Communication/prompt_log.md` with timestamp and conversation summary
2. Check last commit timestamp and message
3. Analyze changed files since last commit
4. Propose synthetic commit message in semantic format
5. Wait for user approval or refinement

Example interaction:
```
User: "I want to commit and update prompt log"
Copilot: [Updates prompt_log.md]
         "Last commit was 2025-11-09 11:26 'refine ui style'. 
         Changed files: theme.ts, AppLayout.tsx, Navigation.tsx, DESIGN_SYSTEM.md
         Proposed: 'docs(webui): complete Hostinger-inspired design system documentation'
         Proceed? (yes/modify)"
```

### PR Ritual
- **Summary:** Brief description of changes
- **Evidence:** Test results, eval scores, or demo link
- **Demo Link:** Loom or screenshot showing feature in action

### Review Etiquette
- Respect the vibe coding loop
- Focus on clarity and correctness, not verbosity
- Comment on quality gates (tests, evals, security)

### Async Transparency
- Share demo results after each micro-cycle
- Update status in project tracker or chat
- Post metrics/screenshots for visibility

### Prompt Logging
**File:** `docs/Communication/prompt_log.md`

**Significant prompts only** are logged with:
- **Timestamp:** YYYY-MM-DD HH:MM format
- **Full prompt text:** No ellipsis or truncation
- **Synthetic answer:** 1-2 sentences max, focus on outcome

**What to log:**
- Architectural decisions, feature requests, workflow changes
- Configuration updates, deployment instructions
- Design decisions, API contracts

**What to skip:**
- Clarifications, acknowledgments, trivial fixes
- Routine debugging, typo corrections

**User trigger:** Say **"I want to commit and update prompt log"** to update both prompt log and initiate commit workflow in one step.

**For Copilot:** When user requests commit + prompt log update, first add log entry (if conversation was significant), then proceed with commit workflow. This creates a conversation audit trail for context continuity.

---

## 8. Typical Workflow Example
1. **Define Goal:** "Improve summarizer precision by 5%."
2. **Implement:** Update LangGraph decision node.
3. **Run Evals:** `make eval`; verify accuracy + latency.
4. **Document:** `make docs`; AI updates project overview.
5. **Demo:** Post dashboard screenshot + short Loom.
6. **Merge:** All gates pass → merge → tag release.

---

## 9. Benefits
- **Predictability:** Each loop produces measurable value.
- **Traceability:** Every artifact is linked to metrics and validation evidence.
- **Scalability:** Reusable process across projects and teams.
- **Confidence:** Rapid delivery without technical debt.

---

## 10. Glossary of Technical Terms and Acronyms
| Term | Definition |
|------|-------------|
| **Vibe Coding** | A development method emphasizing short, high-energy cycles of planning, building, evaluating, and documenting. |
| **Quality Gate** | Mandatory checkpoint ensuring outputs meet objective quality criteria. |
| **Eval / Golden Set** | Predefined test cases measuring reliability, accuracy, or consistency of AI workflows. |
| **CI/CD** | Continuous Integration / Continuous Deployment: automated testing and release pipelines. |
| **p95 Latency** | 95th percentile latency — the time within which 95% of requests complete. Public-facing reports treat this as a supporting diagnostic; the major indicator is AI contribution stability & reliability. |
| **PII (Personally Identifiable Information)** | Sensitive user data that must be masked or protected. |
| **LLM (Large Language Model)** | AI model trained on large text datasets for reasoning and text generation. |
| **LangGraph** | Python framework for stateful agentic workflows. |
| **FastAPI** | High-performance Python web framework for building APIs. |
| **Telemetry** | Collection of system data (metrics, logs, traces) for observability. |
| **Semantic Commit** | Git convention that encodes change intent (feat, fix, refactor, etc.). |
| **Monorepo** | A single repository hosting multiple components of the system. |
| **HITL (Human-in-the-Loop)** | Human supervision step for AI validation and correction. |
| **p95 / p99** | Performance percentiles used to measure latency and stability. |
| **AI contribution stability & reliability** | Consistency of AI outputs across runs and time; sensitivity to model/prompt/data drift; rate of HITL escalations and corrections. |
| **KPI (Key Performance Indicator)** | Metric used to evaluate system or project success. |
| **RAG (Retrieval-Augmented Generation)** | Combining document retrieval with AI generation for context-rich answers. |
| **PR (Pull Request)** | Git-based workflow step to review and merge code contributions. |

---

## 11. Repository hygiene and project layout (living standard)

Keep the repo tidy and predictable so work stays fast and onboarding is painless. Use this structure and conventions:

- Top-level folders only
	- `api/` — FastAPI boundary service (Python). Dockerfile and requirements live here.
	- `auth/` — Auth service (Python). Owns `auth` schema and manual SQL migrations.
	- `postgres/` — Private Postgres container project and SQL/admin docs.
	- `netshell/` — Minimal network shell container for private debugging.
	- `deploy/` — Docker Compose, Traefik config, and environment examples. Provider-specific subfolders (e.g., `hostinger/`).
	- `docs/` — All documentation. Subfolders:
		- `Inception/` — strategy and architecture docs (Positioning, Platform Architecture, Method, Lifecycle, Ops, GTM).
		- `Communication/` — public-facing content (`Post/`, carousels, HTML assets, PlantUML/SVG).
	- `project/` — Conversation logs and working notes.
	- `.git/` and essential dotfiles only at root. No stray files.

- Rules
	- No loose files in the repository root — place everything under the folders above.
	- Co-locate small assets with their docs (images, puml, html) under `docs/Communication/Post/`.
	- Environment files: keep examples (`.env.example`) in `deploy/`; never commit live secrets.
	- One reverse proxy (Traefik) per host; API/UI containers do not publish host ports by default.
	- Prefer same-origin (`/api`) routing to avoid CORS; if using a subdomain for API, enforce a strict CORS allow-list.
	- When deployment flow changes, update `docs/deploy_*` with a short "How to run".
	- Conversation logs: keep chronologically named notes under `project/conversation/` using the pattern `YYYY-MM-DD HH.mm - topic.md`. Each note includes "What the user asked" and "Assistant's response and actions".
	- Migrations: prefer per-service, transactional migrations owned by the service via manual SQL. Avoid shared, global DB state.

- Maintenance checklist (verify on each PR)
	- [ ] New files live under the correct folder (api/deploy/docs), not in repo root
	- [ ] No secrets committed; examples provided instead
	- [ ] Diagrams/figures live near the doc they serve
	- [ ] Compose and Traefik labels updated if routes/hosts changed
	- [ ] Indicators and WRM language stay consistent across docs

---

### Summary
The **Vibe + Quality Gates** method unites creativity and control — empowering developers to iterate fast while guaranteeing measurable reliability. Every cycle creates visible progress, every gate enforces trust, and every release builds both confidence and velocity.

